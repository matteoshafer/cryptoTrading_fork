{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T20:00:58.078129Z",
     "start_time": "2025-05-07T20:00:56.885074Z"
    }
   },
   "source": [
    "from random import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import *\n",
    "from CONSTANTS import *\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:00:58.083337Z",
     "start_time": "2025-05-07T20:00:58.080636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LLM(nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize, layers):\n",
    "        super(LLM, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.last_attention_scores = []\n",
    "        self.embedding = nn.Linear(inputSize, hiddenSize)\n",
    "        \n",
    "        # Hidden layers with attention\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hiddenSize, hiddenSize) for _ in range(layers)\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanisms\n",
    "        self.attention_weights = nn.ModuleList([\n",
    "            nn.Linear(hiddenSize, 1) for _ in range(layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(hiddenSize, 1)\n",
    "        \n",
    "        # Activation\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initial embedding\n",
    "        x = self.activation(self.embedding(x))\n",
    "    \n",
    "        # Dictionary to store attention scores for each layer\n",
    "        attention_scores_dict = {}\n",
    "    \n",
    "        # Process through hidden layers with attention\n",
    "        for i, (layer, attention) in enumerate(zip(self.hidden_layers, self.attention_weights)):\n",
    "            # Calculate attention weights\n",
    "            scores = torch.softmax(attention(x), dim=1)\n",
    "            self.last_attention_scores.append(scores)\n",
    "            attention_scores_dict[f\"layer_{i}\"] = scores  # Store scores in the dictionary\n",
    "    \n",
    "            # Apply attention weights\n",
    "            x = layer(x) * scores\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "    \n",
    "        # Final output\n",
    "        output = self.output(x)\n",
    "    \n",
    "        # Attach attention scores to the output\n",
    "        return output, attention_scores_dict\n",
    "        \n",
    "    def get_attention_weights(self):\n",
    "        \"\"\"Return the attention weights from the last forward pass\"\"\"\n",
    "        return self.last_attention_scores"
   ],
   "id": "1bd6642b4b0d389d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sample",
   "id": "6399c64fd20ca0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:00:58.481329Z",
     "start_time": "2025-05-07T20:00:58.128059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "input_size = 5  # Number of previous numbers in the sequence\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "model = LLM(input_size, hidden_size, num_layers)\n",
    "lossFunction = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "data = [\n",
    "    ([1, 2, 3, 4, 5], 6),\n",
    "    ([2, 3, 4, 5, 6], 7),\n",
    "    ([3, 4, 5, 6, 7], 8),\n",
    "    ([4, 5, 6, 7, 8], 9),\n",
    "]\n",
    "\n",
    "# Convert data to tensors\n",
    "inputs = torch.tensor([item[0] for item in data], dtype=torch.float32)\n",
    "targets = torch.tensor([item[1] for item in data], dtype=torch.float32).unsqueeze(1)\n",
    "print('input shape:', inputs.shape)\n",
    "print('target shape:', targets.shape)\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(inputs)\n",
    "    loss = lossFunction(predictions, targets) ** 0.5\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item()\n",
    "    losses.append(loss)\n",
    "    if loss < 1e-2:\n",
    "        print(f'Early Stopping at Epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# Test the model\n",
    "test_input = torch.tensor([[5, 6, 7, 8, 9]], dtype=torch.float32)\n",
    "predicted_output = model(test_input).item()\n",
    "print(f\"Predicted next number: {predicted_output:.2f}\")\n",
    "pd.Series(losses).plot(color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')"
   ],
   "id": "2b8559d5ad7fa4f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([4, 5])\n",
      "target shape: torch.Size([4, 1])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 27\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[0;32m---> 27\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mlossFunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0.5\u001B[39m\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;66;03m# Backward pass and optimization\u001B[39;00m\n\u001B[1;32m     30\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:535\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m    534\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 535\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:3355\u001B[0m, in \u001B[0;36mmse_loss\u001B[0;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, target):\n\u001B[1;32m   3352\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   3353\u001B[0m         mse_loss, (\u001B[38;5;28minput\u001B[39m, target), \u001B[38;5;28minput\u001B[39m, target, size_average\u001B[38;5;241m=\u001B[39msize_average, reduce\u001B[38;5;241m=\u001B[39mreduce, reduction\u001B[38;5;241m=\u001B[39mreduction\n\u001B[1;32m   3354\u001B[0m     )\n\u001B[0;32m-> 3355\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m()):\n\u001B[1;32m   3356\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   3357\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a target size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) that is different to the input size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3358\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3359\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure they have the same size.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3360\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m   3361\u001B[0m     )\n\u001B[1;32m   3362\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Try the NN for the full data\n",
    "\n",
    "Preprocess bitcoin data"
   ],
   "id": "47e34922ea80a0bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:00:58.485219Z",
     "start_time": "2025-05-07T20:00:36.055653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = setup('BTC')[0]\n",
    "data.set_index('time', inplace=True, drop=True)\n",
    "data"
   ],
   "id": "7251add3a40e2360",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 low      high      open     close        volume   change  \\\n",
       "time                                                                        \n",
       "2024-05-08  60851.04  63013.05  62315.75  61169.53   7486.425968 -1146.22   \n",
       "2024-05-09  60601.60  63424.14  61169.53  63073.57   8360.055382  1904.04   \n",
       "2024-05-10  60150.00  63470.00  63073.55  60787.47  11511.129910 -2286.08   \n",
       "2024-05-11  60450.13  61482.00  60787.99  60814.63   2338.068108    26.64   \n",
       "2024-05-12  60576.05  61843.45  60814.64  61453.02   2694.975779   638.38   \n",
       "...              ...       ...       ...       ...           ...      ...   \n",
       "2025-05-03  95765.13  96974.78  96929.81  95861.33   2077.556760 -1068.48   \n",
       "2025-05-04  94151.67  96312.51  95865.47  94272.55   2834.396865 -1592.92   \n",
       "2025-05-05  93500.01  95218.82  94272.54  94733.99   5180.158939   461.45   \n",
       "2025-05-06  93363.28  96916.25  94733.99  96839.17   6163.867558  2105.18   \n",
       "2025-05-07  96207.28  97738.05  96843.84  97061.99   4431.918646   218.15   \n",
       "\n",
       "            pct_change      SMA_20      SMA_50        EMA_20  ...  \\\n",
       "time                                                          ...   \n",
       "2024-05-08   -1.839374  65890.2210  66417.9682  64677.302333  ...   \n",
       "2024-05-09    3.112726  66247.8435  66426.8854  65046.541526  ...   \n",
       "2024-05-10   -3.624467  66472.6375  66371.6810  65254.222740  ...   \n",
       "2024-05-11    0.043824  66850.1930  66373.6450  65724.407239  ...   \n",
       "2024-05-12    1.049714  67183.0820  66410.7176  66241.225895  ...   \n",
       "...                ...         ...         ...           ...  ...   \n",
       "2025-05-03   -1.102323  72647.6545  69787.8322  76875.959487  ...   \n",
       "2025-05-04   -1.661620  71251.6105  69155.3058  74877.499433  ...   \n",
       "2025-05-05    0.489485  69965.3730  68533.2834  72835.915163  ...   \n",
       "2025-05-06    2.222201  68693.1570  67843.8636  70530.854654  ...   \n",
       "2025-05-07    0.225260  67274.8845  67142.8744  67761.558302  ...   \n",
       "\n",
       "            Unnamed: 0.1  Unnamed: 0  title  date  link  text  sentiment  \\\n",
       "time                                                                       \n",
       "2024-05-08           0.0         0.0      -   NaN     -     -          -   \n",
       "2024-05-09           0.0         0.0      -   NaN     -     -          -   \n",
       "2024-05-10           0.0         0.0      -   NaN     -     -          -   \n",
       "2024-05-11           0.0         0.0      -   NaN     -     -          -   \n",
       "2024-05-12           0.0         0.0      -   NaN     -     -          -   \n",
       "...                  ...         ...    ...   ...   ...   ...        ...   \n",
       "2025-05-03           0.0         0.0      -   NaN     -     -          -   \n",
       "2025-05-04           0.0         0.0      -   NaN     -     -          -   \n",
       "2025-05-05           0.0         0.0      -   NaN     -     -          -   \n",
       "2025-05-06           0.0         0.0      -   NaN     -     -          -   \n",
       "2025-05-07           0.0         0.0      -   NaN     -     -          -   \n",
       "\n",
       "            score  gradient  TextType  \n",
       "time                                   \n",
       "2024-05-08    0.0      0.00     tweet  \n",
       "2024-05-09    0.0   1904.04     tweet  \n",
       "2024-05-10    0.0  -2286.10     tweet  \n",
       "2024-05-11    0.0     27.16     tweet  \n",
       "2024-05-12    0.0    638.39     tweet  \n",
       "...           ...       ...       ...  \n",
       "2025-05-03    0.0  -1068.48     tweet  \n",
       "2025-05-04    0.0  -1588.78     tweet  \n",
       "2025-05-05    0.0    461.44     tweet  \n",
       "2025-05-06    0.0   2105.18     tweet  \n",
       "2025-05-07    0.0    222.82     tweet  \n",
       "\n",
       "[437 rows x 37 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>change</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "      <th>gradient</th>\n",
       "      <th>TextType</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-05-08</th>\n",
       "      <td>60851.04</td>\n",
       "      <td>63013.05</td>\n",
       "      <td>62315.75</td>\n",
       "      <td>61169.53</td>\n",
       "      <td>7486.425968</td>\n",
       "      <td>-1146.22</td>\n",
       "      <td>-1.839374</td>\n",
       "      <td>65890.2210</td>\n",
       "      <td>66417.9682</td>\n",
       "      <td>64677.302333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-09</th>\n",
       "      <td>60601.60</td>\n",
       "      <td>63424.14</td>\n",
       "      <td>61169.53</td>\n",
       "      <td>63073.57</td>\n",
       "      <td>8360.055382</td>\n",
       "      <td>1904.04</td>\n",
       "      <td>3.112726</td>\n",
       "      <td>66247.8435</td>\n",
       "      <td>66426.8854</td>\n",
       "      <td>65046.541526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1904.04</td>\n",
       "      <td>tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-10</th>\n",
       "      <td>60150.00</td>\n",
       "      <td>63470.00</td>\n",
       "      <td>63073.55</td>\n",
       "      <td>60787.47</td>\n",
       "      <td>11511.129910</td>\n",
       "      <td>-2286.08</td>\n",
       "      <td>-3.624467</td>\n",
       "      <td>66472.6375</td>\n",
       "      <td>66371.6810</td>\n",
       "      <td>65254.222740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2286.10</td>\n",
       "      <td>tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-11</th>\n",
       "      <td>60450.13</td>\n",
       "      <td>61482.00</td>\n",
       "      <td>60787.99</td>\n",
       "      <td>60814.63</td>\n",
       "      <td>2338.068108</td>\n",
       "      <td>26.64</td>\n",
       "      <td>0.043824</td>\n",
       "      <td>66850.1930</td>\n",
       "      <td>66373.6450</td>\n",
       "      <td>65724.407239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.16</td>\n",
       "      <td>tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-12</th>\n",
       "      <td>60576.05</td>\n",
       "      <td>61843.45</td>\n",
       "      <td>60814.64</td>\n",
       "      <td>61453.02</td>\n",
       "      <td>2694.975779</td>\n",
       "      <td>638.38</td>\n",
       "      <td>1.049714</td>\n",
       "      <td>67183.0820</td>\n",
       "      <td>66410.7176</td>\n",
       "      <td>66241.225895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>638.39</td>\n",
       "      <td>tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-03</th>\n",
       "      <td>95765.13</td>\n",
       "      <td>96974.78</td>\n",
       "      <td>96929.81</td>\n",
       "      <td>95861.33</td>\n",
       "      <td>2077.556760</td>\n",
       "      <td>-1068.48</td>\n",
       "      <td>-1.102323</td>\n",
       "      <td>72647.6545</td>\n",
       "      <td>69787.8322</td>\n",
       "      <td>76875.959487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1068.48</td>\n",
       "      <td>tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-04</th>\n",
       "      <td>94151.67</td>\n",
       "      <td>96312.51</td>\n",
       "      <td>95865.47</td>\n",
       "      <td>94272.55</td>\n",
       "      <td>2834.396865</td>\n",
       "      <td>-1592.92</td>\n",
       "      <td>-1.661620</td>\n",
       "      <td>71251.6105</td>\n",
       "      <td>69155.3058</td>\n",
       "      <td>74877.499433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1588.78</td>\n",
       "      <td>tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-05</th>\n",
       "      <td>93500.01</td>\n",
       "      <td>95218.82</td>\n",
       "      <td>94272.54</td>\n",
       "      <td>94733.99</td>\n",
       "      <td>5180.158939</td>\n",
       "      <td>461.45</td>\n",
       "      <td>0.489485</td>\n",
       "      <td>69965.3730</td>\n",
       "      <td>68533.2834</td>\n",
       "      <td>72835.915163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>461.44</td>\n",
       "      <td>tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-06</th>\n",
       "      <td>93363.28</td>\n",
       "      <td>96916.25</td>\n",
       "      <td>94733.99</td>\n",
       "      <td>96839.17</td>\n",
       "      <td>6163.867558</td>\n",
       "      <td>2105.18</td>\n",
       "      <td>2.222201</td>\n",
       "      <td>68693.1570</td>\n",
       "      <td>67843.8636</td>\n",
       "      <td>70530.854654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2105.18</td>\n",
       "      <td>tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-07</th>\n",
       "      <td>96207.28</td>\n",
       "      <td>97738.05</td>\n",
       "      <td>96843.84</td>\n",
       "      <td>97061.99</td>\n",
       "      <td>4431.918646</td>\n",
       "      <td>218.15</td>\n",
       "      <td>0.225260</td>\n",
       "      <td>67274.8845</td>\n",
       "      <td>67142.8744</td>\n",
       "      <td>67761.558302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.82</td>\n",
       "      <td>tweet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437 rows × 37 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:00:58.486043Z",
     "start_time": "2025-05-07T20:00:36.531952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequences = []\n",
    "for i in range(len(data)-1):\n",
    "    # create a sequence of all the previous values followed by the next one\n",
    "    # to be separated into different columns at the bottom\n",
    "    sequences.append(sequence(data['gradient'], i))\n",
    "\n",
    "last = data['gradient'].values.tolist() # address the last value\n",
    "item = (last, 0)\n",
    "sequences.append(item)\n",
    "\n",
    "# add it to the data and separate\n",
    "# padding required to make it go through\n",
    "data['list'] = [i[0] for i in sequences]\n",
    "data['list'] = data['list'].apply(lambda a: padding(a, len(data)))\n",
    "data['next'] = [i[1] for i in sequences]\n",
    "data[['gradient', 'list', 'next']]"
   ],
   "id": "14cf25aa7a449f9f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            gradient                                               list  \\\n",
       "time                                                                      \n",
       "2024-05-08      0.00  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2024-05-09   1904.04  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2024-05-10  -2286.10  [0.0, 1904.0400000000009, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2024-05-11     27.16  [0.0, 1904.0400000000009, -2286.0999999999985,...   \n",
       "2024-05-12    638.39  [0.0, 1904.0400000000009, -2286.0999999999985,...   \n",
       "...              ...                                                ...   \n",
       "2025-05-03  -1068.48  [0.0, 1904.0400000000009, -2286.0999999999985,...   \n",
       "2025-05-04  -1588.78  [0.0, 1904.0400000000009, -2286.0999999999985,...   \n",
       "2025-05-05    461.44  [0.0, 1904.0400000000009, -2286.0999999999985,...   \n",
       "2025-05-06   2105.18  [0.0, 1904.0400000000009, -2286.0999999999985,...   \n",
       "2025-05-07    222.82  [0.0, 1904.0400000000009, -2286.0999999999985,...   \n",
       "\n",
       "               next  \n",
       "time                 \n",
       "2024-05-08  1904.04  \n",
       "2024-05-09 -2286.10  \n",
       "2024-05-10    27.16  \n",
       "2024-05-11   638.39  \n",
       "2024-05-12  1479.35  \n",
       "...             ...  \n",
       "2025-05-03 -1588.78  \n",
       "2025-05-04   461.44  \n",
       "2025-05-05  2105.18  \n",
       "2025-05-06   222.82  \n",
       "2025-05-07     0.00  \n",
       "\n",
       "[437 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gradient</th>\n",
       "      <th>list</th>\n",
       "      <th>next</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-05-08</th>\n",
       "      <td>0.00</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1904.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-09</th>\n",
       "      <td>1904.04</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-2286.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-10</th>\n",
       "      <td>-2286.10</td>\n",
       "      <td>[0.0, 1904.0400000000009, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>27.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-11</th>\n",
       "      <td>27.16</td>\n",
       "      <td>[0.0, 1904.0400000000009, -2286.0999999999985,...</td>\n",
       "      <td>638.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-12</th>\n",
       "      <td>638.39</td>\n",
       "      <td>[0.0, 1904.0400000000009, -2286.0999999999985,...</td>\n",
       "      <td>1479.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-03</th>\n",
       "      <td>-1068.48</td>\n",
       "      <td>[0.0, 1904.0400000000009, -2286.0999999999985,...</td>\n",
       "      <td>-1588.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-04</th>\n",
       "      <td>-1588.78</td>\n",
       "      <td>[0.0, 1904.0400000000009, -2286.0999999999985,...</td>\n",
       "      <td>461.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-05</th>\n",
       "      <td>461.44</td>\n",
       "      <td>[0.0, 1904.0400000000009, -2286.0999999999985,...</td>\n",
       "      <td>2105.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-06</th>\n",
       "      <td>2105.18</td>\n",
       "      <td>[0.0, 1904.0400000000009, -2286.0999999999985,...</td>\n",
       "      <td>222.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-07</th>\n",
       "      <td>222.82</td>\n",
       "      <td>[0.0, 1904.0400000000009, -2286.0999999999985,...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Fit the model on the list and next",
   "id": "7f11d55905138b17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preprocessing & Train Test Split",
   "id": "dbad0e17d672c250"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:00:58.486174Z",
     "start_time": "2025-05-07T20:00:38.284921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TRAIN_PCT = 0.8\n",
    "X = data[['list']]\n",
    "y = data['next']\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "splits = tscv.split(X)\n",
    "X_trains = []\n",
    "y_trains = []\n",
    "X_tests = []\n",
    "y_tests = []\n",
    "for train_index, test_index in splits:\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    X_trains.append(X_train)\n",
    "    y_trains.append(y_train)\n",
    "    X_tests.append(X_test)\n",
    "    y_tests.append(y_test)"
   ],
   "id": "e3d47e1a7d10fa67",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:00:58.486277Z",
     "start_time": "2025-05-07T20:00:39.155228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# inputs = torch.tensor(X_train['list'].tolist(), dtype=torch.float32)\n",
    "# targets = torch.tensor(y_train.tolist(), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "metrics = []\n",
    "EPOCHS = 2000\n",
    "NUM_LAYERS = 20\n",
    "HIDDEN_SIZE = 128\n",
    "for X_train, X_test, y_train, y_test in zip(X_trains, X_tests, y_trains, y_tests):\n",
    "    inputs = torch.tensor(X_train['list'].tolist(), dtype=torch.float32)\n",
    "    targets = torch.tensor(y_train.tolist(), dtype=torch.float32).unsqueeze(1)\n",
    "    input_size = len(X_train['list'].tolist()[0])\n",
    "    model = LLM(input_size, HIDDEN_SIZE, NUM_LAYERS)\n",
    "    lossFunction = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    epochs = 100\n",
    "    losses = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Forward pass\n",
    "        predictions = model(inputs)\n",
    "        loss = lossFunction(predictions, targets) ** 0.5 # RMSE\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Get the datetime index from X_test\n",
    "    test_index = X_test.index\n",
    "    \n",
    "    # Make predictions\n",
    "    test_predictions = model(torch.tensor(X_test['list'].tolist(), dtype=torch.float32))\n",
    "    test_loss = lossFunction(test_predictions, torch.tensor(y_test.tolist(), dtype=torch.float32).unsqueeze(1)) ** 0.5\n",
    "    metrics.append(test_loss.item())\n",
    "    \n",
    "    # Convert predictions and targets to pandas Series with the datetime index\n",
    "    pred_series = pd.Series(test_predictions.detach().numpy().flatten(), index=test_index)\n",
    "    target_series = pd.Series(y_test.values, index=test_index)\n",
    "    \n",
    "    # Plot with datetime index\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(pred_series, label='Predictions')\n",
    "    plt.plot(target_series, label='Targets')\n",
    "    plt.legend()\n",
    "    plt.title(f'Test Loss (RMSE): {test_loss.item():.4f}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ],
   "id": "3e282cd2849455e4",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(EPOCHS):\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m     19\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[0;32m---> 20\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mlossFunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;66;03m# RMSE\u001B[39;00m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m# Backward pass and optimization\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:535\u001B[0m, in \u001B[0;36mMSELoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m    534\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 535\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmse_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:3355\u001B[0m, in \u001B[0;36mmse_loss\u001B[0;34m(input, target, size_average, reduce, reduction)\u001B[0m\n\u001B[1;32m   3351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, target):\n\u001B[1;32m   3352\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m   3353\u001B[0m         mse_loss, (\u001B[38;5;28minput\u001B[39m, target), \u001B[38;5;28minput\u001B[39m, target, size_average\u001B[38;5;241m=\u001B[39msize_average, reduce\u001B[38;5;241m=\u001B[39mreduce, reduction\u001B[38;5;241m=\u001B[39mreduction\n\u001B[1;32m   3354\u001B[0m     )\n\u001B[0;32m-> 3355\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m()):\n\u001B[1;32m   3356\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   3357\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a target size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) that is different to the input size (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3358\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3359\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure they have the same size.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   3360\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m   3361\u001B[0m     )\n\u001B[1;32m   3362\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:00:58.486389Z",
     "start_time": "2025-05-06T19:12:05.831304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Input Shape', inputs.shape)\n",
    "print('Target Shape', targets.shape)"
   ],
   "id": "642bc57af0775cd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape torch.Size([365, 437])\n",
      "Target Shape torch.Size([365, 1])\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:00:58.486439Z",
     "start_time": "2025-05-06T19:12:06.993089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 5000\n",
    "losses = []\n",
    "\n",
    "input_size = len(data)  # Number of previous numbers in the sequence\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "model = LLM(input_size, hidden_size, num_layers)\n",
    "lossFunction = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(inputs)\n",
    "    loss = lossFunction(predictions, targets)\n",
    "    loss **= 0.5\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss = loss.item()\n",
    "    losses.append(loss)\n",
    "    if loss < 1e2:\n",
    "        print(f'Early Stopping at Epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "pd.Series(losses).plot()"
   ],
   "id": "5677524033cafad9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:00:58.486504Z",
     "start_time": "2025-05-06T20:05:56.570233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_data = [1,2,3,4,5]\n",
    "test_data = padding(test_data, input_size)\n",
    "test_input = torch.tensor([test_data], dtype=torch.float32)\n",
    "predicted_output = model(test_input).item()\n",
    "predicted_output"
   ],
   "id": "e9042d78778bedc6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "903.7752685546875"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T20:00:58.486551Z",
     "start_time": "2025-05-06T15:15:01.639438Z"
    }
   },
   "cell_type": "code",
   "source": "X_\n",
   "id": "fabf235b16115286",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mpre\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pre' is not defined"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
